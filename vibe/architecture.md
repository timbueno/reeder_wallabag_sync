ðŸ§  Wallabag Feed Sync Script Architecture

This document describes the full architecture of a script designed to synchronize a public JSON feed of article URLs with a userâ€™s Wallabag â€œTo Readâ€ queue. It includes details on how services connect, where state is stored, and how the script behaves when scheduled with cron.

â¸»

ðŸ“ High-Level Overview

The script performs the following tasks:
	1.	Fetches a JSON feed of articles from a public URL.
	2.	Parses the feed to extract article URLs.
	3.	Hashes the URLs using SHA1. These hashes will be called "Given URL Hashes".
	4.	Queries the Wallabag API to retrieve the current â€œTo Readâ€ queue.
	5.	Compares the feed URLs to Wallabag entries:
	â€¢	Compare "Given URL Hashes" with Wallabag responses "given_url_hash".
	â€¢	Adds missing URLs to Wallabag.
	â€¢	Archives URLs in Wallabag that are no longer in the feed.

â¸»

ðŸ§± Architecture Components

1. Cron Scheduler
	â€¢	Role: Triggers the script at scheduled intervals (e.g., hourly).
	â€¢	Tool: cron on Unix-based systems.

2. Feed Source
	â€¢	Type: Publicly accessible JSON endpoint.
	â€¢	Contents: List of article URLs (expected in a consistent format).

3. Script (Main Logic)
	â€¢	Language: Python, Bash, or similar scripting language.
	â€¢	Responsibilities:
	â€¢	Fetch feed.
	â€¢	Normalize and hash URLs.
	â€¢	Authenticate with Wallabag API.
	â€¢	Compare current state and sync.

4. Wallabag API
	â€¢	Endpoints Used:
	â€¢	GET /api/entries: To list current â€œTo Readâ€ items.
	â€¢	POST /api/entries: To add new articles.
	â€¢	PATCH /api/entries/{id}: To archive articles.

5. State Storage
	â€¢	Method 1: Stateless (Preferred)
	â€¢	Hashes are calculated on the fly during each run.
	â€¢	Wallabag is the source of truth.
	â€¢	No persistent state needed.
	â€¢	Method 2: Cached State File (Optional)
	â€¢	Store last known feed SHA1 hashes in a file (e.g., last_feed.json) to avoid unnecessary API calls.
	â€¢	Tradeoff: Additional complexity and file locking concerns.

â¸»

ðŸ”— Data Flow Diagram

graph TD;
    cron[cron schedule]
    script[Sync Script]
    feed[Public JSON Feed]
    wallabag[Wallabag API]
    
    cron --> script
    script --> feed
    script --> wallabag
    wallabag --> script


â¸»

ðŸ§¾ Example Script Flow (Stateless)
	1.	Start: Script runs via cron.
	2.	Fetch Feed: GET request to JSON feed URL.
	3.	Parse & Hash: Extract URLs and hash them with SHA1.
	4.	Fetch Wallabag Entries: GET /api/entries?archive=0&detail=metadata&perPage=500.
	5.	Compare:
	â€¢	Add articles in feed but not in Wallabag.
	â€¢	Archive articles in Wallabag not in the feed.
	6.	End.

â¸»

ðŸ” Authentication

Wallabag requires OAuth2 tokens:
	â€¢	access_token: Used for API requests.
	â€¢	refresh_token: Used to obtain a new access token.
	â€¢	Store these securely (e.g., in .env, or config file with proper permissions).

â¸»

ðŸ“¦ Environment/Config

FEED_URL=https://example.com/feed.json
WALLABAG_BASE_URL=https://wallabag.example.com
WALLABAG_CLIENT_ID=your_client_id
WALLABAG_CLIENT_SECRET=your_client_secret
WALLABAG_USERNAME=your_username
WALLABAG_PASSWORD=your_password


â¸»

ðŸ“‹ Cron Example

# Run every hour
0 * * * * /usr/bin/python3 /path/to/wallabag_sync.py >> /var/log/wallabag_sync.log 2>&1


â¸»

ðŸ“ˆ Optional Enhancements
	â€¢	Logging: Store actions (added/archived URLs) in a log file.
	â€¢	Retry logic for failed API calls.
	â€¢	Error reporting via email, webhook, or push notification.
	â€¢	Unit tests for the hashing and comparison logic.

â¸»

âœ… Summary

This script enables automated curation of Wallabag reading material by keeping it in sync with a dynamic feed. It avoids state persistence by relying on hashing and API queries, making it simple, robust, and cron-friendly.

â¸»
